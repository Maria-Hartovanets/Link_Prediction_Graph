{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "#!pip install -q torch-scatter~=2.1.0 torch-sparse~=0.6.16 torch-cluster~=1.6.0 torch-spline-conv~=1.2.1 torch-geometric==2.2.0 -f https://data.pyg.org/whl/torch-{torch.__version__}.html\n",
    "\n",
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed(0)\n",
    "torch.cuda.manual_seed_all(0)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graph Autoencoder (VAE) and Variational Graph Autoencoder (VGAE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "import torch\n",
    "torch.manual_seed(0)\n",
    "import matplotlib.pyplot as plt\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.datasets import Planetoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "transform = T.Compose([\n",
    "    T.NormalizeFeatures(),\n",
    "    T.ToDevice(device),\n",
    "    T.RandomLinkSplit(num_val=0.05, num_test=0.1, is_undirected=True, split_labels=True, add_negative_train_samples=False),\n",
    "])\n",
    "\n",
    "dataset = Planetoid('.', name='PubMed', transform=transform)\n",
    "\n",
    "train_data, val_data, test_data = dataset[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data set[0]:\n",
      " (Data(x=[19717, 500], edge_index=[2, 75352], y=[19717], train_mask=[19717], val_mask=[19717], test_mask=[19717], pos_edge_label=[37676], pos_edge_label_index=[2, 37676]), Data(x=[19717, 500], edge_index=[2, 75352], y=[19717], train_mask=[19717], val_mask=[19717], test_mask=[19717], pos_edge_label=[2216], pos_edge_label_index=[2, 2216], neg_edge_label=[2216], neg_edge_label_index=[2, 2216]), Data(x=[19717, 500], edge_index=[2, 79784], y=[19717], train_mask=[19717], val_mask=[19717], test_mask=[19717], pos_edge_label=[4432], pos_edge_label_index=[2, 4432], neg_edge_label=[4432], neg_edge_label_index=[2, 4432]))\n",
      "Train data: \n",
      "Data(x=[19717, 500], edge_index=[2, 75352], y=[19717], train_mask=[19717], val_mask=[19717], test_mask=[19717], pos_edge_label=[37676], pos_edge_label_index=[2, 37676])\n",
      "Val data: \n",
      "Data(x=[19717, 500], edge_index=[2, 75352], y=[19717], train_mask=[19717], val_mask=[19717], test_mask=[19717], pos_edge_label=[2216], pos_edge_label_index=[2, 2216], neg_edge_label=[2216], neg_edge_label_index=[2, 2216])\n",
      "Test data: \n",
      "Data(x=[19717, 500], edge_index=[2, 79784], y=[19717], train_mask=[19717], val_mask=[19717], test_mask=[19717], pos_edge_label=[4432], pos_edge_label_index=[2, 4432], neg_edge_label=[4432], neg_edge_label_index=[2, 4432])\n"
     ]
    }
   ],
   "source": [
    "print(f\"Data set[0]:\\n {dataset[0]}\")\n",
    "print(f\"Train data: \\n{train_data}\")\n",
    "print(f\"Val data: \\n{val_data}\")\n",
    "print(f\"Test data: \\n{test_data}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import GCNConv, VGAE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(torch.nn.Module):\n",
    "    def __init__(self, dim_in, dim_out):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(dim_in, 2 * dim_out)\n",
    "        self.conv_mu = GCNConv(2 * dim_out, dim_out)\n",
    "        self.conv_logstd = GCNConv(2 * dim_out, dim_out)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        return self.conv_mu(x, edge_index), self.conv_logstd(x, edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0 | Loss: 3.4529 | Val AUC: 0.8899 | Val AP: 0.8688\n",
      "Epoch 10 | Loss: 1.7083 | Val AUC: 0.8908 | Val AP: 0.8635\n",
      "Epoch 20 | Loss: 1.3152 | Val AUC: 0.8952 | Val AP: 0.8697\n",
      "Epoch 30 | Loss: 1.2523 | Val AUC: 0.8940 | Val AP: 0.8731\n",
      "Epoch 40 | Loss: 1.2273 | Val AUC: 0.8890 | Val AP: 0.8724\n",
      "Epoch 50 | Loss: 1.2018 | Val AUC: 0.8992 | Val AP: 0.8838\n",
      "Epoch 60 | Loss: 1.1425 | Val AUC: 0.9129 | Val AP: 0.9048\n",
      "Epoch 70 | Loss: 1.0751 | Val AUC: 0.8794 | Val AP: 0.8811\n",
      "Epoch 80 | Loss: 1.0632 | Val AUC: 0.8899 | Val AP: 0.8906\n",
      "Epoch 90 | Loss: 1.0622 | Val AUC: 0.8965 | Val AP: 0.8953\n",
      "Epoch 100 | Loss: 1.0493 | Val AUC: 0.8932 | Val AP: 0.8930\n",
      "Test AUC: 0.8932 | Test AP 0.8930\n"
     ]
    }
   ],
   "source": [
    "model = VGAE(Encoder(dataset.num_features, 16)).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    z = model.encode(train_data.x, train_data.edge_index)\n",
    "    loss = model.recon_loss(z, train_data.pos_edge_label_index) + (1 / train_data.num_nodes) * model.kl_loss()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return float(loss)\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(data):\n",
    "    model.eval()\n",
    "    z = model.encode(data.x, data.edge_index)\n",
    "    return model.test(z, data.pos_edge_label_index, data.neg_edge_label_index)\n",
    "\n",
    "for epoch in range(101):\n",
    "    loss = train()\n",
    "    val_auc, val_ap = test(test_data)\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch {epoch:>2} | Loss: {loss:.4f} | Val AUC: {val_auc:.4f} | Val AP: {val_ap:.4f}') \n",
    "\n",
    "test_auc, test_ap = test(test_data) \n",
    "print(f'Test AUC: {test_auc:.4f} | Test AP {test_ap:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5314, 0.5059, 0.4966,  ..., 0.5013, 0.4787, 0.5042],\n",
       "        [0.5059, 0.6615, 0.5817,  ..., 0.5772, 0.5542, 0.5781],\n",
       "        [0.4966, 0.5817, 0.5432,  ..., 0.5382, 0.5331, 0.5391],\n",
       "        ...,\n",
       "        [0.5013, 0.5772, 0.5382,  ..., 0.5366, 0.5262, 0.5366],\n",
       "        [0.4787, 0.5542, 0.5331,  ..., 0.5262, 0.5373, 0.5255],\n",
       "        [0.5042, 0.5781, 0.5391,  ..., 0.5366, 0.5255, 0.5376]],\n",
       "       grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = model.encode(test_data.x, test_data.edge_index) \n",
    "Ahat = torch.sigmoid(z @ z.T)\n",
    "Ahat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SEAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from scipy.sparse.csgraph import shortest_path\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Conv1d, MaxPool1d, Linear, Dropout, BCEWithLogitsLoss\n",
    "\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.transforms import RandomLinkSplit\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import GCNConv, aggr\n",
    "from torch_geometric.utils import k_hop_subgraph, to_scipy_sparse_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data: \n",
      "Data(x=[19717, 500], edge_index=[2, 53192], y=[19717], train_mask=[19717], val_mask=[19717], test_mask=[19717], pos_edge_label=[26596], pos_edge_label_index=[2, 26596], neg_edge_label=[26596], neg_edge_label_index=[2, 26596])\n",
      "Val data: \n",
      "Data(x=[19717, 500], edge_index=[2, 53192], y=[19717], train_mask=[19717], val_mask=[19717], test_mask=[19717], pos_edge_label=[8864], pos_edge_label_index=[2, 8864], neg_edge_label=[8864], neg_edge_label_index=[2, 8864])\n",
      "Test data: \n",
      "Data(x=[19717, 500], edge_index=[2, 70920], y=[19717], train_mask=[19717], val_mask=[19717], test_mask=[19717], pos_edge_label=[8864], pos_edge_label_index=[2, 8864], neg_edge_label=[8864], neg_edge_label_index=[2, 8864])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load PubMed dataset\n",
    "transform = RandomLinkSplit(num_val=0.2, num_test=0.2,  is_undirected=True, split_labels=True)\n",
    "dataset = Planetoid('.', name='PubMed', transform=transform)\n",
    "\n",
    "\n",
    "train_data, val_data, test_data = dataset[0]\n",
    "#print(f\"Data set[0]:\\n {dataset[0]}\")\n",
    "print(f\"Train data: \\n{train_data}\")\n",
    "print(f\"Val data: \\n{val_data}\")\n",
    "print(f\"Test data: \\n{test_data}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seal_processing(dataset, edge_label_index, y):\n",
    "    data_list = []\n",
    "\n",
    "    for src, dst in edge_label_index.t().tolist():\n",
    "        sub_nodes, sub_edge_index, mapping, _ = k_hop_subgraph([src, dst], 2, dataset.edge_index, relabel_nodes=True)\n",
    "        src, dst = mapping.tolist()\n",
    "\n",
    "        # Remove target link from the subgraph\n",
    "        mask1 = (sub_edge_index[0] != src) | (sub_edge_index[1] != dst)\n",
    "        mask2 = (sub_edge_index[0] != dst) | (sub_edge_index[1] != src)\n",
    "        sub_edge_index = sub_edge_index[:, mask1 & mask2]\n",
    "\n",
    "        # Double-radius node labeling (DRNL)\n",
    "        src, dst = (dst, src) if src > dst else (src, dst)\n",
    "        adj = to_scipy_sparse_matrix(sub_edge_index, num_nodes=sub_nodes.size(0)).tocsr()\n",
    "\n",
    "        idx = list(range(src)) + list(range(src + 1, adj.shape[0]))\n",
    "        adj_wo_src = adj[idx, :][:, idx]\n",
    "\n",
    "        idx = list(range(dst)) + list(range(dst + 1, adj.shape[0]))\n",
    "        adj_wo_dst = adj[idx, :][:, idx]\n",
    "\n",
    "        # Calculate the distance between every node and the source target node\n",
    "        d_src = shortest_path(adj_wo_dst, directed=False, unweighted=True, indices=src)\n",
    "        d_src = np.insert(d_src, dst, 0, axis=0)\n",
    "        d_src = torch.from_numpy(d_src)\n",
    "\n",
    "        # Calculate the distance between every node and the destination target node\n",
    "        d_dst = shortest_path(adj_wo_src, directed=False, unweighted=True, indices=dst-1)\n",
    "        d_dst = np.insert(d_dst, src, 0, axis=0)\n",
    "        d_dst = torch.from_numpy(d_dst)\n",
    "\n",
    "        # Calculate the label z for each node\n",
    "        dist = d_src + d_dst\n",
    "        z = 1 + torch.min(d_src, d_dst) + dist // 2 * (dist // 2 + dist % 2 - 1)\n",
    "        z[src], z[dst], z[torch.isnan(z)] = 1., 1., 0.\n",
    "        z = z.to(torch.long)\n",
    "\n",
    "        # Concatenate node features and one-hot encoded node labels (with a fixed number of classes)\n",
    "        node_labels = F.one_hot(z, num_classes=200).to(torch.float)\n",
    "        node_emb = dataset.x[sub_nodes]\n",
    "        node_x = torch.cat([node_emb, node_labels], dim=1)\n",
    "\n",
    "        # Create data object\n",
    "        data = Data(x=node_x, z=z, edge_index=sub_edge_index, y=y)\n",
    "        data_list.append(data)\n",
    "\n",
    "    return data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enclosing subgraphs extraction\n",
    "train_pos_data_list = seal_processing(train_data, train_data.pos_edge_label_index, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 19716 is out of bounds for dimension 0 with size 19712",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_9492\\1670088302.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain_neg_data_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mseal_processing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mneg_edge_label_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_9492\\4030845921.py\u001b[0m in \u001b[0;36mseal_processing\u001b[1;34m(dataset, edge_label_index, y)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdst\u001b[0m \u001b[1;32min\u001b[0m \u001b[0medge_label_index\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m         \u001b[0msub_nodes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msub_edge_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmapping\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mk_hop_subgraph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0medge_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrelabel_nodes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m         \u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdst\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmapping\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Maria\\anaconda3\\lib\\site-packages\\torch_geometric\\utils\\subgraph.py\u001b[0m in \u001b[0;36mk_hop_subgraph\u001b[1;34m(node_idx, num_hops, edge_index, relabel_nodes, num_nodes, flow)\u001b[0m\n\u001b[0;32m    296\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_hops\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    297\u001b[0m         \u001b[0mnode_mask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfill_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 298\u001b[1;33m         \u001b[0mnode_mask\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msubsets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    299\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex_select\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0medge_mask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    300\u001b[0m         \u001b[0msubsets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0medge_mask\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: index 19716 is out of bounds for dimension 0 with size 19712"
     ]
    }
   ],
   "source": [
    "train_neg_data_list = seal_processing(train_data, train_data.neg_edge_label_index, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 18962 is out of bounds for dimension 0 with size 15643",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_9492\\347447517.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mval_pos_data_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mseal_processing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpos_edge_label_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mval_neg_data_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mseal_processing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mneg_edge_label_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_9492\\4030845921.py\u001b[0m in \u001b[0;36mseal_processing\u001b[1;34m(dataset, edge_label_index, y)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdst\u001b[0m \u001b[1;32min\u001b[0m \u001b[0medge_label_index\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m         \u001b[0msub_nodes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msub_edge_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmapping\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mk_hop_subgraph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0medge_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrelabel_nodes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m         \u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdst\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmapping\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Maria\\anaconda3\\lib\\site-packages\\torch_geometric\\utils\\subgraph.py\u001b[0m in \u001b[0;36mk_hop_subgraph\u001b[1;34m(node_idx, num_hops, edge_index, relabel_nodes, num_nodes, flow)\u001b[0m\n\u001b[0;32m    296\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_hops\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    297\u001b[0m         \u001b[0mnode_mask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfill_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 298\u001b[1;33m         \u001b[0mnode_mask\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msubsets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    299\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex_select\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0medge_mask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    300\u001b[0m         \u001b[0msubsets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0medge_mask\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: index 18962 is out of bounds for dimension 0 with size 15643"
     ]
    }
   ],
   "source": [
    "\n",
    "val_pos_data_list = seal_processing(val_data, val_data.pos_edge_label_index, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_neg_data_list = seal_processing(val_data, val_data.neg_edge_label_index, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pos_data_list = seal_processing(test_data, test_data.pos_edge_label_index, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_neg_data_list = seal_processing(test_data, test_data.neg_edge_label_index, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_pos_data_list + train_neg_data_list\n",
    "val_dataset = val_pos_data_list + val_neg_data_list\n",
    "test_dataset = test_pos_data_list + test_neg_data_list\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DGCNN(torch.nn.Module):\n",
    "    def __init__(self, dim_in, k=30):\n",
    "        super().__init__()\n",
    "\n",
    "        # GCN layers\n",
    "        self.gcn1 = GCNConv(dim_in, 32)\n",
    "        self.gcn2 = GCNConv(32, 32)\n",
    "        self.gcn3 = GCNConv(32, 32)\n",
    "        self.gcn4 = GCNConv(32, 1)\n",
    "\n",
    "        # Global sort pooling\n",
    "        self.global_pool = aggr.SortAggregation(k=k)\n",
    "\n",
    "        # Convolutional layers\n",
    "        self.conv1 = Conv1d(1, 16, 97, 97)\n",
    "        self.conv2 = Conv1d(16, 32, 5, 1)\n",
    "        self.maxpool = MaxPool1d(2, 2)\n",
    "\n",
    "        # Dense layers\n",
    "        self.linear1 = Linear(352, 128)\n",
    "        self.dropout = Dropout(0.5)\n",
    "        self.linear2 = Linear(128, 1)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        # 1. Graph Convolutional Layers\n",
    "        h1 = self.gcn1(x, edge_index).tanh()\n",
    "        h2 = self.gcn2(h1, edge_index).tanh()\n",
    "        h3 = self.gcn3(h2, edge_index).tanh()\n",
    "        h4 = self.gcn4(h3, edge_index).tanh()\n",
    "        h = torch.cat([h1, h2, h3, h4], dim=-1)\n",
    "\n",
    "        # 2. Global sort pooling\n",
    "        h = self.global_pool(h, batch)\n",
    "\n",
    "        # 3. Traditional convolutional and dense layers\n",
    "        h = h.view(h.size(0), 1, h.size(-1))\n",
    "        h = self.conv1(h).relu()\n",
    "        h = self.maxpool(h)\n",
    "        h = self.conv2(h).relu()\n",
    "        h = h.view(h.size(0), -1)\n",
    "        h = self.linear1(h).relu()\n",
    "        h = self.dropout(h)\n",
    "        h = self.linear2(h).sigmoid()\n",
    "\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = DGCNN(train_dataset[0].num_features).to(device)\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=0.0001)\n",
    "criterion = BCEWithLogitsLoss()\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data.x, data.edge_index, data.batch)\n",
    "        loss = criterion(out.view(-1), data.y.to(torch.float))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += float(loss) * data.num_graphs\n",
    "\n",
    "    return total_loss / len(train_dataset)\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "    y_pred, y_true = [], []\n",
    "\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        out = model(data.x, data.edge_index, data.batch)\n",
    "        y_pred.append(out.view(-1).cpu())\n",
    "        y_true.append(data.y.view(-1).cpu().to(torch.float))\n",
    "\n",
    "    auc = roc_auc_score(torch.cat(y_true), torch.cat(y_pred))\n",
    "    ap = average_precision_score(torch.cat(y_true), torch.cat(y_pred))\n",
    "\n",
    "    return auc, ap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0 | Loss: 0.7020 | Val AUC: 0.7103 | Val AP: 0.7293\n",
      "Epoch  1 | Loss: 0.6404 | Val AUC: 0.7782 | Val AP: 0.8128\n",
      "Epoch  2 | Loss: 0.6120 | Val AUC: 0.7767 | Val AP: 0.8128\n",
      "Epoch  3 | Loss: 0.6073 | Val AUC: 0.7773 | Val AP: 0.8161\n",
      "Epoch  4 | Loss: 0.6046 | Val AUC: 0.7773 | Val AP: 0.8175\n",
      "Epoch  5 | Loss: 0.6033 | Val AUC: 0.7827 | Val AP: 0.8206\n",
      "Epoch  6 | Loss: 0.6015 | Val AUC: 0.7829 | Val AP: 0.8231\n",
      "Epoch  7 | Loss: 0.5997 | Val AUC: 0.7846 | Val AP: 0.8286\n",
      "Epoch  8 | Loss: 0.5971 | Val AUC: 0.7840 | Val AP: 0.8227\n",
      "Epoch  9 | Loss: 0.5961 | Val AUC: 0.7800 | Val AP: 0.8256\n",
      "Epoch 10 | Loss: 0.5949 | Val AUC: 0.7808 | Val AP: 0.8111\n",
      "Epoch 11 | Loss: 0.5933 | Val AUC: 0.7794 | Val AP: 0.8041\n",
      "Epoch 12 | Loss: 0.5930 | Val AUC: 0.7808 | Val AP: 0.8019\n",
      "Epoch 13 | Loss: 0.5906 | Val AUC: 0.7810 | Val AP: 0.8036\n",
      "Epoch 14 | Loss: 0.5911 | Val AUC: 0.7768 | Val AP: 0.8064\n",
      "Epoch 15 | Loss: 0.5898 | Val AUC: 0.7769 | Val AP: 0.8008\n",
      "Epoch 16 | Loss: 0.5885 | Val AUC: 0.7789 | Val AP: 0.8040\n",
      "Epoch 17 | Loss: 0.5875 | Val AUC: 0.7800 | Val AP: 0.8058\n",
      "Epoch 18 | Loss: 0.5870 | Val AUC: 0.7784 | Val AP: 0.8041\n",
      "Epoch 19 | Loss: 0.5861 | Val AUC: 0.7808 | Val AP: 0.8045\n",
      "Epoch 20 | Loss: 0.5873 | Val AUC: 0.7766 | Val AP: 0.8027\n",
      "Epoch 21 | Loss: 0.5872 | Val AUC: 0.7774 | Val AP: 0.8025\n",
      "Epoch 22 | Loss: 0.5858 | Val AUC: 0.7764 | Val AP: 0.7975\n",
      "Epoch 23 | Loss: 0.5850 | Val AUC: 0.7809 | Val AP: 0.8052\n",
      "Epoch 24 | Loss: 0.5847 | Val AUC: 0.7790 | Val AP: 0.8014\n",
      "Epoch 25 | Loss: 0.5842 | Val AUC: 0.7805 | Val AP: 0.8066\n",
      "Epoch 26 | Loss: 0.5836 | Val AUC: 0.7818 | Val AP: 0.8049\n",
      "Epoch 27 | Loss: 0.5825 | Val AUC: 0.7783 | Val AP: 0.8017\n",
      "Epoch 28 | Loss: 0.5819 | Val AUC: 0.7789 | Val AP: 0.8028\n",
      "Epoch 29 | Loss: 0.5818 | Val AUC: 0.7823 | Val AP: 0.8030\n",
      "Epoch 30 | Loss: 0.5811 | Val AUC: 0.7786 | Val AP: 0.8032\n",
      "Test AUC: 0.7773 | Test AP 0.7943\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for epoch in range(31):\n",
    "    loss = train()\n",
    "    val_auc, val_ap = test(val_loader)\n",
    "    print(f'Epoch {epoch:>2} | Loss: {loss:.4f} | Val AUC: {val_auc:.4f} | Val AP: {val_ap:.4f}')\n",
    "\n",
    "test_auc, test_ap = test(test_loader)\n",
    "print(f'Test AUC: {test_auc:.4f} | Test AP {test_ap:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
